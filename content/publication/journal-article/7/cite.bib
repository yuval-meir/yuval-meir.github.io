@article{KORESH2024129909,
title = {Scaling in Deep and Shallow Learning Architectures},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {646},
pages = {129909},
year = {2024},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2024.129909},
url = {https://www.sciencedirect.com/science/article/pii/S0378437124004187},
author = {Ella Koresh and Tal Halevi and Yuval Meir and Dolev Dilmoney and Tamar Dror and Ronit Gross and Ofek Tevet and Shiri Hodassman and Ido Kanter},
keywords = {Deep learning, Machine learning, Shallow learning},
abstract = {The realization of classification tasks using deep learning is a primary goal of artificial intelligence; however, its possible universal behavior remains unexplored. Herein, we demonstrate a scaling behavior for the test error, ϵ, as a function of the number of classified labels, K. For trained utmost deep architectures on CIFAR-100 ϵ(K)∝Kρ with ρ∼1, and in case of reduced deep architectures, ρ continuously decreases until a crossover to ϵ(K)∝log(K) is observed for shallow architectures. A similar crossover is observed for shallow architectures, where the number of filters in the convolutional layers is proportionally increased. This unified the scaling behavior of deep and shallow architectures, which yields a reduced latency method. The dependence of Δϵ/ΔK on the trained architecture is expected to be crucial in learning scenarios involving dynamic number of labels.}
}